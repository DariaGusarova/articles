# Hybrid Cosine Based Convolutional Neural Networks

1. Link: https://arxiv.org/abs/1904.01987
2. Website: https://imatge.upc.edu

## Мотивация

CNNs показывают хорошие результаты во многих практических областях, в том числе в задачах классификации и сегментации изображений. Но общее количество параметров таких моделей может быть довольно большим. Если ограничиться рассмотрением сверточных слоев, то можно заметить, что каждый такой слой несет пропорциональное размеру ядра свертки число параметров. 
С другой стороны широко известны дискретные косинусные преобразования, они использутся при обрабоке изображений, например, в стандарте JPEG. Также они могут быть использованы над выходами сверточных слоев нейронных сетей для ускорения обучения (https://ieeexplore.ieee.org/document/7026196 и https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/0786.pdf)

## Основная идея

Перестать хранить веса каждой свертки, а восстанавливать их из параметров, используя как базовую функцию косинус (функция от номера столбца, строки в ядре свертки). Под частотными параметрами подразумевается амплитуда, фаза и частота. Тогда получится, что для каждой свертки будет хранится список параметров, который не зависит от конкретного размера ядра, и количество параметров уменьшится.

## Пространственные параметры

В статье рассматриваются два варианта задания пространственной составляющей ядра свертки: spatial-product и spatial-direction.  Первый соответствует произведению двух косинусоид от номера строки и номера столбца по отдельности, второй представляется косинусоидой от номера строки и номера столбца одновременно, то есть задает направление на прямоугольном ядре.

## Признаковые параметры

Здесь рассматриваются также два варианта: вводить косинусоиду от номера признака или же считать, что для каждого признака значение выбирается независимо.

## Итоговый блок

Ядро новой свертки получается из произведения пространственной составляющей и признаковой составляющей. При построении блока, который заменяет обычный сверточный слой, обычно комбинируется часть признаков, полученных через стандартную свертку, и часть признаков, полученных модифицированной сверткой. При этом доля признаков второго типа равна некоторому параметру альфа.

## Эксперименты

Эксперименты проводятся на моделях VGG16bn (https://arxiv.org/abs/1409.1556) и ResNet50 (https://arxiv.org/abs/1512.03385) и датасете CIFAR10, который состоит из 60000 цветных картинок размера 32x32 и 10 классов. В качестве бейзлайна используется соотвествующая стандартная архитектура.

Первый эксперимент сравнивает различные пространственные компоненты, считая, что признаковая компонента равна feature-weights. Результаты довольно схожи, но в случае spatial-product немного лучше. В общем можно считать, что в независимости от выбора компоненты модель получает качество близкое к безлайну.

Второй эксперимент сравнивает различные признаковые компоненты, считая, что пространственная компонента равна spatial-product. Здесь ситуация сильно различается, feature-direct показывает хороший коэффициет сжатия, но плохой итоговый результат, так что кажется более осмысленным в дальнейшем использовать feature-weight.

В третьем эксперименте рассматриваются два варианта параметра альфа: 1 или 0.5, и разный признаковые составляющие. В итоге получается, что при использовании коэффициента 1 итоговое качество заметно хуже бейзлайна, то есть имеет смысл сохранять в модели часть стандартных сверток. С точки зрения признаковой составляющей опять выигрывает feature-weights.

## Финальный эксперимент

Архитектуры сетей используются теже, пространсвенная характеристика фиксируется в spatial-product, параметр альфа берется как 0.5. И используются еще два новых датасета: CIFAR100 (почти тоже самое, что и CIFAR, только данные из 100 классов) и датасет Monkeys, в нем 1400 цветных картинок из 10 классов, но их размер заметно больше 400x300.

В финальном эксперименте основное внимание обращается на 4 параметра: итоговая точность модели по валидационному датасету, номер эпохи после которой достигается некоторый порог качества, количество параметров модели и коэффициент сжатия от базовой модели.

По CIFAR10 можно заметить, что качество модифицированных сеток почти не отличается от базовой. На CIFAR100 наблюдается схожая ситуация. По Monkeys также можно заметить, что если использовать feature-weight, то скорость сходимости сильно выше скорости сходимости базовой сетки. В тоже время качество модифицированных сеток не хуже.

## Заключение

В итоге получается, что точность сеток с гибридными слоями почти не отличается от базовой, зато происходит сжатие числа параметров почти в два раза. Также заметно ускоряется сходимость и в новых сетках можно по желанию увеличивать ядра свероток без увеличения общего числа параметров.
